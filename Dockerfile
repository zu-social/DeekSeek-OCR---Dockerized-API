# DeepSeek-OCR vLLM Docker Image
# Based on official vLLM OpenAI image for better compatibility

FROM vllm/vllm-openai:v0.8.5

# Switch to root user to install packages
USER root

# Set working directory
WORKDIR /app

# Copy the DeepSeek-OCR vLLM implementation
COPY DeepSeek-OCR/DeepSeek-OCR-master/DeepSeek-OCR-vllm/ ./DeepSeek-OCR-vllm/

# Copy the startup script
COPY start_server.py .

# Copy requirements file and install additional dependencies
COPY DeepSeek-OCR/requirements.txt .

# Install Python dependencies excluding conflicting packages
RUN pip install --no-cache-dir \
    PyMuPDF \
    img2pdf \
    einops \
    easydict \
    addict \
    Pillow \
    numpy

# Install additional dependencies for the API server
RUN pip install --no-cache-dir \
    fastapi==0.104.1 \
    uvicorn[standard]==0.24.0 \
    python-multipart==0.0.6

# Install flash-attn for optimal performance (if not already included)
RUN pip install --no-cache-dir flash-attn==2.7.3 --no-build-isolation || echo "flash-attn may already be installed"

# Downgrade tokenizers to compatible version if needed
RUN pip install --no-cache-dir tokenizers==0.13.3 || echo "Using existing tokenizers version"

# Add the DeepSeek-OCR directory to PYTHONPATH
ENV PYTHONPATH="/app/DeepSeek-OCR-vllm:${PYTHONPATH}"

# Create directories for outputs
RUN mkdir -p /app/outputs

# Make the startup script executable
RUN chmod +x /app/start_server.py

# Expose the API port
EXPOSE 8000

# Set the default command to use our custom server
# Override the entrypoint to run our script directly
# Use the full path to python to avoid PATH issues
ENTRYPOINT ["/usr/bin/python3", "/app/start_server.py"]